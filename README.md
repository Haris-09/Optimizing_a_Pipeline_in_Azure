# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**the problem statement:**
- The dataset provided in this project is UCI Bank Marketing Dataset. This dataset contains information about financial and personal details of customers like job, martial status, education, salary, etc. The y colum or the labels indicates whether the customer subscribed to the fixed term deposit or not. This depicts that this is a classifiacation broblem with two classes or a binary classification problem.

- For this problem we used two approaches HyperDrive and AutoML. Finally we compared the performance.

**the solution:**
- Using the HyperDrive i got an accuracy of *0.9144157814871017*
- Using the AutoML the best model is *StandardScalerWrapper LightGBM* which gives an accuracy of *0.90434*

## Scikit-learn Pipeline
**The pipeline architecture**
- First we start with the train.py script. this script is used for data preperation. first we get the dataset from the given URL using TabularDatasetFactory. Then we clean the data using clean_data function which uses one hot encoding method. After that splited the data into training and testing set. The script uses logistic regression as a classification alogrithm. Logistic Regression has two arguments regularization strength and no of iterations to converge by default they are defined as 1.0 and 100 respectively.
- Then i created a Standard_D2_V2 Compute Instance named Compute-CPU to run the notebook.
- In the notebook we first initializes the exsisting worksape object and created a new experiment to track all the runs in the workspace.
- Then created a Standard_D2_V2 compute cluster with 4 nodes to train the model using the ComputeTarget.
- Next part is the hyperparameter selection and specifying a policy. The two hyperparameters used are C regulizaion strength and max_iter maximum number of iterations to converge for the logistic regression algorithm. I used random parameter sampling to sample over a discrete set of values. the parameter search space used for C is (0.002, 0.02, 0.2, 2.0) and max_iter is (100, 200, 300, 500). I used the Bandit policy. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.
- Then i created HyperDriveConfig by passing estimator, policy, hyperparameter sampling and primary metric name on which our model will be measured. I used Accuracy as a primary metric.
- Finally best model is saved which gives the higher accuracy.

**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
