# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**the problem statement:**
- The dataset provided in this project is UCI Bank Marketing Dataset. This dataset contains information about financial and personal details of customers like job, martial status, education, salary, etc. The y colum or the labels indicates whether the customer subscribed to the fixed term deposit or not. This depicts that this is a classifiacation broblem with two classes or a binary classification problem.

- For this problem we used two approaches HyperDrive and AutoML. Finally we compared the performance.

**the solution:**
- Using the HyperDrive i got an accuracy of *0.9144157814871017*
- Using the AutoML the best model is *StandardScalerWrapper LightGBM* which gives an accuracy of *0.90434*

## Scikit-learn Pipeline
**The pipeline architecture**
- First we start with the train.py script. this script is used for data preperation. first we get the dataset from the given URL using TabularDatasetFactory. Then we clean the data using clean_data function which uses one hot encoding method. After that splited the data into training and testing set. The script uses logistic regression as a classification alogrithm. Logistic Regression has two arguments regularization strength and no of iterations to converge by default they are defined as 1.0 and 100 respectively.
- Then i created a Standard_D2_V2 Compute Instance named Compute-CPU to run the notebook.
- In the notebook we first initializes the exsisting worksape object and created a new experiment to track all the runs in the workspace.
- Then created a Standard_D2_V2 compute cluster with 4 nodes to train the model using the ComputeTarget.
- Next part is the hyperparameter selection and specifying a policy. The two hyperparameters used are C regulizaion strength and max_iter maximum number of iterations to converge for the logistic regression algorithm. I used random parameter sampling to sample over a discrete set of values. the parameter search space used for C is (0.002, 0.02, 0.2, 2.0) and max_iter is (100, 200, 300, 500). I used the Bandit policy. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.
- Then i created HyperDriveConfig by passing estimator, policy, hyperparameter sampling and primary metric name on which our model will be measured. I used Accuracy as a primary metric.
- Finally best model is saved which gives the higher accuracy.

**benefits of the parameter sampler**
the parameter sampler used is Random Sampling. In random sampling, hyperparameter values are randomly selected from among the given search space. Random sampling is chosen because it supports discrete and continuous hyperparameters, and early termination of low-performance runs.

The hyperparameters to be optimized in the Logistic Regression algorithm are --C and --max_iter. Here C value is the Inverse of regularization strength smaller the value stronger the regularization. RandomParameterSampling allows different combinations of both parameters when on the run and from the multiple runs best run will be selected which has the higher accuracy. A continuous range of values is provided for the Random Sampler to choose from. max_iter is the maximum number of iterations it takes for the optimization algorithm to converge. A list of equally spaced options for Max Iterations is provided. From this combination of hyperparameter options, HyperDrive randomly chooses one for each to then find hyperparameter leading to optimal results.

**benefits of the early stopping policy**
Termination policy helps to save time by stopping runs in case of poor performance. Which also helps in saving compute resources. I used the Bandit policy. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run. We can also specify evaluation_interval which is the frequency for applying the policy, and delay_evaluation which delays the first policy evaluation for a specified number of intervals.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
